{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from shutil import copyfile\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torch.utils import model_zoo\n",
    "\n",
    "from pytoune.framework import Model, ModelCheckpoint, BestModelRestore, CSVLogger\n",
    "from pytoune import torch_to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_dataset(path):\n",
    "    tgz_filename = \"images.tgz\"\n",
    "    urllib.request.urlretrieve(\"http://www.vision.caltech.edu/visipedia-data/CUB-200/images.tgz\", tgz_filename)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    archive = tarfile.open(tgz_filename)\n",
    "    archive.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy(source_path, filenames, dest_path):\n",
    "    for filename in filenames:\n",
    "        source = os.path.join(source_path, filename)\n",
    "        dest = os.path.join(dest_path, filename)\n",
    "        copyfile(source, dest)\n",
    "\n",
    "def split_train_valid_test(dataset_path, train_path, valid_path, test_path, train_split=0.6, valid_split=0.2): # test_split=0.2\n",
    "    np.random.seed(42)\n",
    "    for classname in sorted(os.listdir(dataset_path)):\n",
    "        if classname.startswith('.'):\n",
    "            continue\n",
    "        train_class_path = os.path.join(train_path, classname)\n",
    "        valid_class_path = os.path.join(valid_path, classname)\n",
    "        test_class_path = os.path.join(test_path, classname)\n",
    "\n",
    "        os.makedirs(train_class_path, exist_ok=True)\n",
    "        os.makedirs(valid_class_path, exist_ok=True)\n",
    "        os.makedirs(test_class_path, exist_ok=True)\n",
    "\n",
    "        dataset_class_path = os.path.join(dataset_path, classname)\n",
    "        filenames = sorted(filename for filename in os.listdir(dataset_class_path) if not filename.startswith('.'))\n",
    "        np.random.shuffle(filenames)\n",
    "\n",
    "        num_examples = len(filenames)\n",
    "        train_last_idx = math.ceil(num_examples*train_split)\n",
    "        valid_first_idx = train_last_idx + math.floor(num_examples*valid_split)\n",
    "        train_filenames = filenames[0:train_last_idx]\n",
    "        valid_filenames = filenames[train_last_idx:valid_first_idx]\n",
    "        test_filenames = filenames[valid_first_idx:]\n",
    "        copy(dataset_class_path, train_filenames, train_class_path)\n",
    "        copy(dataset_class_path, valid_filenames, valid_class_path)\n",
    "        copy(dataset_class_path, test_filenames, test_class_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the split train/valid/test.\n",
    "\n",
    "base_path = './CUB200'\n",
    "dataset_path = os.path.join(base_path, 'images')\n",
    "train_path = os.path.join(base_path, 'train')\n",
    "valid_path = os.path.join(base_path, 'valid')\n",
    "test_path = os.path.join(base_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_dataset(base_path)\n",
    "split_train_valid_test(dataset_path, train_path, valid_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "n_epoch = 30\n",
    "num_classes = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the PyTorch's datasets for our problem.\n",
    "\n",
    "norm_coefs = {}\n",
    "norm_coefs['cub200'] = [(0.47421962,  0.4914721 ,  0.42382449), (0.22846779,  0.22387765,  0.26495799)]\n",
    "norm_coefs['imagenet'] = [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*norm_coefs['cub200'])\n",
    "])\n",
    "\n",
    "train_set = ImageFolder(train_path, transform=transform)\n",
    "valid_set = ImageFolder(valid_path, transform=transform)\n",
    "test_set = ImageFolder(test_path, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading a pretrained ResNet-18 networks and replacing \n",
    "# the head with the number of neurons equal to our number \n",
    "# of classes.\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We freeze the network except for its head.\n",
    "\n",
    "def freeze_weights(resnet18):\n",
    "    for name, param in resnet18.named_parameters():\n",
    "        if not name.startswith('fc.'):\n",
    "            param.require_grads = False\n",
    "\n",
    "freeze_weights(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One nice feature of Pytoune is callbacks.\n",
    "\n",
    "callbacks = [\n",
    "    # Save the latest weights to be able to continue the optimization at the end for more epochs.\n",
    "    ModelCheckpoint('last_epoch.ckpt', temporary_filename='last_epoch.ckpt.tmp'),\n",
    "    \n",
    "    # Save the weights in a new file when the current model is better than all previous models.\n",
    "    ModelCheckpoint('best_epoch_{epoch}.ckpt', monitor='val_acc', mode='max', save_best_only=True, restore_best=True, verbose=True, temporary_filename='best_epoch.ckpt.tmp'),\n",
    "    \n",
    "    # Save the losses and accuracies for each epoch in a TSV.\n",
    "    CSVLogger('log.tsv', separator='\\t'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we start the training and output its final test \n",
    "# loss and accuracy.\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.SGD(resnet18.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Pytoune Model\n",
    "model = Model(resnet18, optimizer, loss_function, metrics=['accuracy'])\n",
    "\n",
    "# Send model on GPU\n",
    "model.to(device)\n",
    "\n",
    "# Train\n",
    "model.fit_generator(train_loader, valid_loader, epochs=n_epoch, callbacks=callbacks)\n",
    "\n",
    "# Test\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logs = pd.read_csv('log.tsv', sep='\\t')\n",
    "print(logs)\n",
    "\n",
    "best_epoch_idx = logs['val_acc'].idxmax()\n",
    "best_epoch = int(logs.loc[best_epoch_idx]['epoch'])\n",
    "print(\"Best epoch: %d\" % best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore best model from checkpoint and test it.\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=False, num_classes=num_classes)\n",
    "\n",
    "model = Model(resnet18, None, nn.CrossEntropyLoss(), metrics=['accuracy'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.load_weights('best_epoch_{epoch}.ckpt'.format(epoch=best_epoch))\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
