{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "from pytoune.framework import Model, ModelCheckpoint, CSVLogger\n",
    "from pytoune import torch_to_numpy\n",
    "from pytoune.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "\n",
    "cuda_device = 0\n",
    "device = torch.device(\"cuda:%d\" % cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "train_split_percent = 0.8\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "n_epoch = 5\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST dataset and creating DataLoaders.\n",
    "\n",
    "train_dataset = MNIST('./mnist/', train=True, download=True, transform=transforms.ToTensor())\n",
    "valid_dataset = MNIST('./mnist/', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = MNIST('./mnist/', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "num_data = len(train_dataset)\n",
    "indices = list(range(num_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = math.floor(train_split_percent * num_data)\n",
    "\n",
    "train_indices = indices[:split]\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "\n",
    "valid_indices = indices[split:]\n",
    "valid_dataset = Subset(valid_dataset, valid_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fully_connected_network():\n",
    "    return nn.Sequential(\n",
    "        Flatten(),\n",
    "        nn.Linear(28*28, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, num_classes)\n",
    "    )\n",
    "\n",
    "def create_convolutional_network():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 32, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout(0.25),\n",
    "        Flatten(),\n",
    "        nn.Linear(64*14*14 , 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, num_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name, pytorch_module):\n",
    "    # One nice feature of Pytoune is callbacks.\n",
    "    callbacks = [\n",
    "        # Save the latest weights to be able to continue the optimization at the end for more epochs.\n",
    "        ModelCheckpoint(name + '_last_epoch.ckpt', temporary_filename='last_epoch.ckpt.tmp'),\n",
    "\n",
    "        # Save the weights in a new file when the current model is better than all previous models.\n",
    "        ModelCheckpoint(name + '_best_epoch_{epoch}.ckpt', monitor='val_acc', mode='max', save_best_only=True, restore_best=True, verbose=False, temporary_filename='best_epoch.ckpt.tmp'),\n",
    "\n",
    "        # Save the losses and accuracies for each epoch in a TSV.\n",
    "        CSVLogger(name + '_log.tsv', separator='\\t'),\n",
    "    ]\n",
    "    \n",
    "    # Finally, we start the training and output its final test \n",
    "    # loss and accuracy.\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = optim.SGD(pytorch_module.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Pytoune Model\n",
    "    fc_model = Model(pytorch_module, optimizer, loss_function, metrics=['accuracy'])\n",
    "\n",
    "    # Send model on GPU\n",
    "    fc_model.to(device)\n",
    "\n",
    "    # Train\n",
    "    fc_model.fit_generator(train_loader, valid_loader, epochs=n_epoch, callbacks=callbacks)\n",
    "\n",
    "    # Test\n",
    "    test_loss, test_acc = fc_model.evaluate_generator(test_loader)\n",
    "    print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize my network\n",
    "fc_net = create_fully_connected_network()\n",
    "print(fc_net)\n",
    "\n",
    "# Start training\n",
    "train('fc', fc_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize my network\n",
    "cnn = create_convolutional_network()\n",
    "print(cnn)\n",
    "\n",
    "# Start training\n",
    "train('cnn', cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
